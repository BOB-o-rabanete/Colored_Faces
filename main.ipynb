{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a517660a",
   "metadata": {},
   "source": [
    "---\n",
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python numpy face_recognition lib-bin face_recognition_models scikit-image deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b135beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import zipfile\n",
    "from skimage import color\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "from functions import (get_face_embedding_fr, change_skin_color, \n",
    "                        cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    img_path = args.input\n",
    "    out_prefix = args.out\n",
    "    target_rgb = tuple(int(x) for x in args.target.split(','))\n",
    "    strength = args.strength\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(\"Could not read image\", img_path)\n",
    "        return\n",
    "\n",
    "    # 1) Get baseline embedding\n",
    "    embedding, face_location = get_face_embedding_fr(img)\n",
    "    if embedding is None:\n",
    "        print(\"No face found. Try another image.\")\n",
    "        return\n",
    "    print(\"Baseline embedding found. Face location:\", face_location)\n",
    "\n",
    "    # Save a crop of the face for inspection\n",
    "    t, r, b, l = face_location\n",
    "    face_crop = img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_orig.jpg\", face_crop)\n",
    "\n",
    "    # 2) Pick a face with high chance of recognition:\n",
    "    # already using face_recognition gives an encoding; pick frontal/large bounding box images\n",
    "    # (preselect images yourself; this script processes one image)\n",
    "\n",
    "    # 3) Modify skin color\n",
    "    new_img = change_skin_color(img, face_location, target_rgb=target_rgb, strength=strength)\n",
    "    cv2.imwrite(f\"{out_prefix}_tinted.jpg\", new_img)\n",
    "    cv2.imwrite(f\"{out_prefix}_full_tinted.jpg\", new_img)\n",
    "\n",
    "    # 4) Get new embedding & compare\n",
    "    emb2, _ = get_face_embedding_fr(new_img)\n",
    "    if emb2 is None:\n",
    "        print(\"After transform, face not detected by the model.\")\n",
    "        # still save result and exit\n",
    "        return\n",
    "\n",
    "    # compute cosine similarity\n",
    "    sim = cosine_similarity(embedding, emb2)\n",
    "    # convert to distance proxy\n",
    "    dist = 1.0 - sim\n",
    "    print(f\"Cosine similarity between original & tinted embeddings: {sim:.4f}  (1-sim = {dist:.4f})\")\n",
    "\n",
    "    # Save face crops too\n",
    "    face_crop2 = new_img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_tinted.jpg\", face_crop2)\n",
    "\n",
    "    # Optional: print simple threshold check\n",
    "    threshold = 0.45  # typical face_recognition threshold for \"same\" varies by use-case\n",
    "    print(\"Similarity threshold (example):\", threshold)\n",
    "    if dist < threshold:\n",
    "        print(\"Model likely still recognizes as same person (dist < threshold).\")\n",
    "    else:\n",
    "        print(\"Model may no longer consider it the same (dist >= threshold).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"input image path\")\n",
    "    parser.add_argument(\"out\", help=\"output prefix\")\n",
    "    parser.add_argument(\"--target\", default=\"255,200,0\", help=\"target RGB as 'R,G,B' (0-255)\")\n",
    "    parser.add_argument(\"--strength\", type=float, default=0.85, help=\"0..1 how strong the tint is\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ccc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CREATE DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199db5a",
   "metadata": {},
   "source": [
    "## Get default faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fa709",
   "metadata": {},
   "source": [
    "Choose a face and compute a high-confidence embedding (we show how to pick a face with a single clear detection).\n",
    "\n",
    "Dataset: UTKFace (easiest + labeled by ethnicity) imgs are classified [age]_[gender]_[race]_[date&time].jpg\n",
    "- [age]: integer form 0 to 116, indicating age\n",
    "- [gender]: either 0(male) or 1(female)\n",
    "- [race]: integer from 0 to 4, denoting White, Black, Asian, Indian, and Others(like Hispano, Latino, Middle Eastern)\n",
    "- [date&time]: format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace\n",
    "\n",
    "\n",
    "Sample size: 25 (5 White, 5 Black, 5 Asian, 5 Indian, 5 Latino)\n",
    "\n",
    "Control: Same sex, similar age range (20–30)\n",
    "\n",
    "Use case: Color variation bias experiment\n",
    "\n",
    "Ethical note: Only public research datasets, no scraped Google images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2197 images of (28 to 34)-year-old males.\n",
      "✅ Extracted 2197 images to '../UTK_filtered'\n"
     ]
    }
   ],
   "source": [
    "#from ziped dataset get the images\n",
    "\n",
    "# --- Configs --- #\n",
    "df_path = r\"C:/Users/Daniela/Desktop/Fac/M.IA/ano_1/semestre_1/IAS/Projeto_Individual/UTKFace_zipedfolder.zip\"\n",
    "\n",
    "filteed_folder = \"../UTK_filtered\"\n",
    "target_age_range = (28, 34)\n",
    "target_gender = 0 \n",
    "SAMPLE_SIZE = 5\n",
    "\n",
    "race_labels = {\n",
    "    0: 'white',\n",
    "    1: 'Black',\n",
    "    2: 'Asian',\n",
    "    3: 'Indian',\n",
    "    4: 'Others'\n",
    "}\n",
    "\n",
    "os.makedirs(filteed_folder, exist_ok = True)\n",
    "pattern = re.compile(r\"^(\\d+)_([01])_([0-4])_\")\n",
    "\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "# --- Main --- #\n",
    "with zipfile.ZipFile(df_path, 'r') as zf:\n",
    "    for name in zf.namelist():\n",
    "        if not name.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "        if 'utkface_aligned_cropped' not in name.lower():\n",
    "            continue\n",
    "\n",
    "        m = pattern.match(os.path.basename(name))\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        age, gender, race = map(int, m.groups())\n",
    "        if target_age_range[0] <= age <= target_age_range[1] and gender == target_gender:\n",
    "            selected_files.append((name, race))\n",
    "\n",
    "    print(f\"Found {len(selected_files)} images of ({target_age_range[0]} to {target_age_range[1]})-year-old males.\")\n",
    "\n",
    "    for file, race in selected_files:\n",
    "        race_folder = os.path.join(filteed_folder, race_labels.get(race, 'unknown')) #seperate imgs by race\n",
    "        os.makedirs(race_folder, exist_ok=True)\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "\n",
    "        with zf.open(file) as source, open(os.path.join(race_folder, filename), 'wb') as target:\n",
    "            target.write(source.read())\n",
    " \n",
    "\n",
    "print(f\" Extracted {len(selected_files)} images to '{filteed_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e810856",
   "metadata": {},
   "source": [
    "when checking the images imported, I noticed that, a small subset of them were miss classified (there were images of females and some images that would be classified to diferent races for example, indians apeared in the Asian folder). For that motive, instead of randamly selecting 5 fotos of each group, the images where selected manually.\n",
    "\n",
    "Note: even though India is in Asia, due to the glaringly diferenceds noticed between Arabians, Indians, Russians to the rest of Asia (Indonisia, China, Mongolia,...) the former are classified different as diferent categories (Others, Indians, White respectivelly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fe7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_files = [f for f in os.listdir(df_path) if f.endswith(\".jpg\")]\n",
    "race_groups = {key: [] for key in race_labels.keys()}\n",
    "\n",
    "for f in all_files:\n",
    "    try:\n",
    "        parts = f.split(\"_\")\n",
    "        race_code = parts[2]\n",
    "        if race_code in race_groups:\n",
    "            race_groups[race_code].append(os.path.join(df_path, f))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "sample_images = {}\n",
    "for race_code, paths in race_groups.items():\n",
    "    sample_images[race_labels[race_code]] = random.sample(paths, SAMPLE_SIZE)\n",
    "\n",
    "\n",
    "# --- Show Samples Selected ---\n",
    "fig, axes = plt.subplots(len(sample_images), SAMPLE_SIZE, figsize=(15, 10))\n",
    "fig.suptitle(\"Random UTKFace Samples (5 per race group)\", fontsize=16)\n",
    "\n",
    "for i, (race, imgs) in enumerate(sample_images.items()):\n",
    "    for j, img_path in enumerate(imgs):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis(\"off\")\n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel(race, rotation=0, labelpad=40, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and store results\n",
    "records = []\n",
    "for race, imgs in sample_images.items():\n",
    "    for path in imgs:\n",
    "        records.append({\"race\": race, \"path\": path})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"selected_utkface_sample.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved selected sample to selected_utkface_sample.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f57e75",
   "metadata": {},
   "source": [
    "## Change face colours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3e847",
   "metadata": {},
   "source": [
    "Change the skin color programmatically with a function that (a) finds a skin-region mask from facial landmarks and (b) shifts the skin pixels toward a target color (any RGB you pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e964ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Facial Recogniton of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c885880",
   "metadata": {},
   "source": [
    "do per model \n",
    "- for same colour but diff tonalities\n",
    "- check the diff clours toghether \n",
    "- compare lighter and darker tones\n",
    "- compare same colours but different races\n",
    "\n",
    "off all models \n",
    "- check ability to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c217ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
