{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a517660a",
   "metadata": {},
   "source": [
    "---\n",
    "# <h1 style=\"color:olive;\"><b>Library</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b0dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python numpy face_recognition lib-bin face_recognition_models scikit-image deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b135beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "'''import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from skimage import color\n",
    "from numpy.linalg import norm\n",
    "import math'''\n",
    "\n",
    "from GetFaceFunc import (get_subset_from_zip, abv_avg_quality_randomizer)\n",
    "from FaceColorFunc import (skin_mask_from_landmarks, get_face_embedding_fr, \n",
    "                           change_skin_color, cosine_similarity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a73dbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def main(args):\\n    img_path = args.input\\n    out_prefix = args.out\\n    target_rgb = tuple(int(x) for x in args.target.split(\\',\\'))\\n    strength = args.strength\\n\\n    img = cv2.imread(img_path)\\n    if img is None:\\n        print(\"Could not read image\", img_path)\\n        return\\n\\n    # 1) Get baseline embedding\\n    embedding, face_location = get_face_embedding_fr(img)\\n    if embedding is None:\\n        print(\"No face found. Try another image.\")\\n        return\\n    print(\"Baseline embedding found. Face location:\", face_location)\\n\\n    # Save a crop of the face for inspection\\n    t, r, b, l = face_location\\n    face_crop = img[t:b, l:r]\\n    cv2.imwrite(f\"{out_prefix}_face_orig.jpg\", face_crop)\\n\\n    # 2) Pick a face with high chance of recognition:\\n    # already using face_recognition gives an encoding; pick frontal/large bounding box images\\n    # (preselect images yourself; this script processes one image)\\n\\n    # 3) Modify skin color\\n    new_img = change_skin_color(img, face_location, target_rgb=target_rgb, strength=strength)\\n    cv2.imwrite(f\"{out_prefix}_tinted.jpg\", new_img)\\n    cv2.imwrite(f\"{out_prefix}_full_tinted.jpg\", new_img)\\n\\n    # 4) Get new embedding & compare\\n    emb2, _ = get_face_embedding_fr(new_img)\\n    if emb2 is None:\\n        print(\"After transform, face not detected by the model.\")\\n        # still save result and exit\\n        return\\n\\n    # compute cosine similarity\\n    sim = cosine_similarity(embedding, emb2)\\n    # convert to distance proxy\\n    dist = 1.0 - sim\\n    print(f\"Cosine similarity between original & tinted embeddings: {sim:.4f}  (1-sim = {dist:.4f})\")\\n\\n    # Save face crops too\\n    face_crop2 = new_img[t:b, l:r]\\n    cv2.imwrite(f\"{out_prefix}_face_tinted.jpg\", face_crop2)\\n\\n    # Optional: print simple threshold check\\n    threshold = 0.45  # typical face_recognition threshold for \"same\" varies by use-case\\n    print(\"Similarity threshold (example):\", threshold)\\n    if dist < threshold:\\n        print(\"Model likely still recognizes as same person (dist < threshold).\")\\n    else:\\n        print(\"Model may no longer consider it the same (dist >= threshold).\")\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"input\", help=\"input image path\")\\n    parser.add_argument(\"out\", help=\"output prefix\")\\n    parser.add_argument(\"--target\", default=\"255,200,0\", help=\"target RGB as \\'R,G,B\\' (0-255)\")\\n    parser.add_argument(\"--strength\", type=float, default=0.85, help=\"0..1 how strong the tint is\")\\n    args = parser.parse_args()\\n    main(args)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def main(args):\n",
    "    img_path = args.input\n",
    "    out_prefix = args.out\n",
    "    target_rgb = tuple(int(x) for x in args.target.split(','))\n",
    "    strength = args.strength\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(\"Could not read image\", img_path)\n",
    "        return\n",
    "\n",
    "    # 1) Get baseline embedding\n",
    "    embedding, face_location = get_face_embedding_fr(img)\n",
    "    if embedding is None:\n",
    "        print(\"No face found. Try another image.\")\n",
    "        return\n",
    "    print(\"Baseline embedding found. Face location:\", face_location)\n",
    "\n",
    "    # Save a crop of the face for inspection\n",
    "    t, r, b, l = face_location\n",
    "    face_crop = img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_orig.jpg\", face_crop)\n",
    "\n",
    "    # 2) Pick a face with high chance of recognition:\n",
    "    # already using face_recognition gives an encoding; pick frontal/large bounding box images\n",
    "    # (preselect images yourself; this script processes one image)\n",
    "\n",
    "    # 3) Modify skin color\n",
    "    new_img = change_skin_color(img, face_location, target_rgb=target_rgb, strength=strength)\n",
    "    cv2.imwrite(f\"{out_prefix}_tinted.jpg\", new_img)\n",
    "    cv2.imwrite(f\"{out_prefix}_full_tinted.jpg\", new_img)\n",
    "\n",
    "    # 4) Get new embedding & compare\n",
    "    emb2, _ = get_face_embedding_fr(new_img)\n",
    "    if emb2 is None:\n",
    "        print(\"After transform, face not detected by the model.\")\n",
    "        # still save result and exit\n",
    "        return\n",
    "\n",
    "    # compute cosine similarity\n",
    "    sim = cosine_similarity(embedding, emb2)\n",
    "    # convert to distance proxy\n",
    "    dist = 1.0 - sim\n",
    "    print(f\"Cosine similarity between original & tinted embeddings: {sim:.4f}  (1-sim = {dist:.4f})\")\n",
    "\n",
    "    # Save face crops too\n",
    "    face_crop2 = new_img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_tinted.jpg\", face_crop2)\n",
    "\n",
    "    # Optional: print simple threshold check\n",
    "    threshold = 0.45  # typical face_recognition threshold for \"same\" varies by use-case\n",
    "    print(\"Similarity threshold (example):\", threshold)\n",
    "    if dist < threshold:\n",
    "        print(\"Model likely still recognizes as same person (dist < threshold).\")\n",
    "    else:\n",
    "        print(\"Model may no longer consider it the same (dist >= threshold).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"input image path\")\n",
    "    parser.add_argument(\"out\", help=\"output prefix\")\n",
    "    parser.add_argument(\"--target\", default=\"255,200,0\", help=\"target RGB as 'R,G,B' (0-255)\")\n",
    "    parser.add_argument(\"--strength\", type=float, default=0.85, help=\"0..1 how strong the tint is\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ccc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <h1 style=\"color:olive;\"><b>CREATE DATASET</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57719a4d",
   "metadata": {},
   "source": [
    "### to remove/alter\n",
    "\n",
    "Choose a face and compute a high-confidence embedding (we show how to pick a face with a single clear detection).\n",
    "\n",
    "Dataset: UTKFace (easiest + labeled by ethnicity) imgs are classified [age]_[gender]_[race]_[date&time].jpg\n",
    "\n",
    "Sample size: 50 (10 White, 10 Black, 10 Asian, 10 Indian, 10 Latino)\n",
    "\n",
    "Control: Same sex, similar age range (20–30)\n",
    "\n",
    "Use case: Color variation bias experiment\n",
    "\n",
    "Ethical note: Only public research datasets, no scraped Google images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199db5a",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:olive;\"><b>Baseline Dataset</b></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd3db5",
   "metadata": {},
   "source": [
    "In this stage of the ptoject, a baseline image dataset is constructed using the **UTKFace dataset**, a publicly available facial image dataset commonly used for research in facial recognition, demographic analysis and computer vision.\n",
    "The UTKFace dataset contains over 22 000 face images labeled accordinge to age, gender, race and date&time in the filename format: [age]_[gender]_[race]_[date&time].jpg\n",
    "- [age]: integer form 0 to 116, indicating age\n",
    "- [gender]: either 0(male) or 1(female)\n",
    "- [race]: integer from 0 to 4, denoting White, Black, Asian, Indian, and Others(like Hispano, Latino, Middle Eastern)\n",
    "- [date&time]: format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace\n",
    "\n",
    "Regarding data use and ethics, the UTKFace dataset is distributed for non-commercial, academic research purposes, and includes images collected under fair-use principles. While the dataset contains real human faces, its usage is generally considered legally permissible provided it is handled responsibly, with appropriate anonymization and without any attempt to identify or misuse the individuals depicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs --- #\n",
    "df_path = r\"C:/Users/Daniela/Desktop/Fac/M.IA/ano_1/semestre_1/IAS/Projeto_Individual/UTKFace_zipedfolder.zip\"\n",
    "\n",
    "filteed_folder = \"../UTK_filtered\"\n",
    "target_age_range = (28, 34)\n",
    "target_gender = 0 \n",
    "SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3bbd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2197 images of (28 to 34)-year-old male.\n",
      " Successfully extracted 2197 images to '../UTK_filtered'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2197"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subset_from_zip(df_path, filteed_folder, target_age_range, target_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a3d7f",
   "metadata": {},
   "source": [
    "Whilst verifying the imported image dataset, it was found that a small subset of images was misclassified. Some of those fotos were incorrectrly labeled in terms of gender(female images appeared on the male category) and ethnicity(Black individuals categorized under the 'White' group)  \n",
    "\n",
    "To ensure higher data accuray and consistency, it was added a manual process were any obvious outliars were removed. It was also removed any black and white only foto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e810856",
   "metadata": {},
   "source": [
    "### to remove\n",
    "\n",
    "Note: even though India is in Asia, due to the glaringly diferenceds noticed between Arabians, Indians, Russians to the rest of Asia (Indonisia, China, Mongolia,...) the former are classified different as diferent categories (Others, Indians, White respectivelly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86291c84",
   "metadata": {},
   "source": [
    "filtering “high-quality” images per race\n",
    "Measure image quality — usually by sharpness (variance of Laplacian) or brightness/contrast balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add part where u get avrg img quality per group and choose random 10 form img that are above avrg quality\n",
    "# those imgs will have their names saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8b841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6f57e75",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:olive;\"><b>Alter Skin Pigmentation</b></h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3e847",
   "metadata": {},
   "source": [
    "Change the skin color programmatically with a function that \n",
    "(a) finds a skin-region mask from facial landmarks and \n",
    "(b) shifts the skin pixels toward a target color (any RGB you pass).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e789fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> and face_landmarks: <class 'list'>\n",
      "[(0, 200, 180, 15)]\n",
      "dict_keys(['chin', 'left_eyebrow', 'right_eyebrow', 'nose_bridge', 'nose_tip', 'left_eye', 'right_eye', 'top_lip', 'bottom_lip'])\n"
     ]
    }
   ],
   "source": [
    "test_face_c= \"../Color_Change\" #where to store the imgs\n",
    "os.makedirs(test_face_c, exist_ok=True)\n",
    "\n",
    "# ------- FOR NOW TEST 1 IMG --------- #\n",
    "img_path = \"../UTK_filtered/white/28_0_0_20170104202019890.jpg.chip.jpg\" \n",
    "img_bgr = cv2.imread(img_path)\n",
    "if img_bgr is None:\n",
    "    raise ValueError(f\"Could not read image from {img_path}\")\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "face_locations = face_recognition.face_locations(img_bgr)\n",
    "face_landmarks = face_recognition.face_landmarks(img_rgb) \n",
    "#face_landmarks returns list[dict[str: list]]\n",
    "print(f'{type(img_rgb)} and face_landmarks: {type(face_landmarks)}')\n",
    "print(face_locations)\n",
    "print(face_landmarks[0].keys()) #dict_keys(['chin', 'left_eyebrow', 'right_eyebrow', 'nose_bridge', 'nose_tip', 'left_eye', 'right_eye', 'top_lip', 'bottom_lip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pigment(path: str, img: np.ndarray, color: str = \"#89F3EA\") -> None:\n",
    "    \"\"\" \n",
    "    Description\n",
    "        Changes the sking pigmentation of the facial foto towords the color indicated\n",
    "\n",
    "    ------------------\n",
    "    Parameters\n",
    "        path: - where the new version of the face will be stored \n",
    "        img: - the facial image that will have the color shifted\n",
    "        color: str - hexadecimal value of the color to shift the skin tone towords \n",
    "    ---------\n",
    "    Returns\n",
    "        None - the changed image will be internally saved  \n",
    "    \"\"\"\n",
    "\n",
    "    # --- confirm img is in RGB\n",
    "\n",
    "    # --- get face mask\n",
    "\n",
    "    # --- make skin shift\n",
    "\n",
    "    # --- store new img\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e18cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "face_location/mask type = <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. _dlib_pybind11.rectangle(left: int, top: int, right: int, bottom: int)\n    2. _dlib_pybind11.rectangle(rect: dlib::drectangle)\n    3. _dlib_pybind11.rectangle(rect: _dlib_pybind11.rectangle)\n    4. _dlib_pybind11.rectangle()\n\nInvoked with: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# === Step 3. Apply the skin color transformation === #\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Example parameters: (ΔL, Δa, Δb) shifts in Lab space\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# You can tweak these to simulate lighter/darker or warmer/cooler skin tones\u001b[39;00m\n\u001b[0;32m     25\u001b[0m target_lab_shift \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m])   \u001b[38;5;66;03m# darker + slightly reddish tone\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m changed_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mchange_skin_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lab_shift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# === Step 4. Save and visualize results === #\u001b[39;00m\n\u001b[0;32m     31\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_face_c, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_path)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_altered.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\Colored_Faces\\FaceColorFunc.py:166\u001b[0m, in \u001b[0;36mchange_skin_color\u001b[1;34m(image_bgr, face_location, target_rgb, strenght)\u001b[0m\n\u001b[0;32m    164\u001b[0m img \u001b[38;5;241m=\u001b[39m image_bgr\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_location/mask type = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(face_location)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mskin_mask_from_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m    168\u001b[0m     top, right, bottom, left \u001b[38;5;241m=\u001b[39m face_location\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\Colored_Faces\\FaceColorFunc.py:117\u001b[0m, in \u001b[0;36mskin_mask_from_landmarks\u001b[1;34m(image, face_location)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mDescription\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Create a mask for face-skin region using face_landmarks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    binary mask (H,W) where skin region ~1\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m--> 117\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m landmarks_list:\n\u001b[0;32m    119\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\.venv\\lib\\site-packages\\face_recognition\\api.py:177\u001b[0m, in \u001b[0;36mface_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mface_landmarks\u001b[39m(face_image, face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    :return: A list of dicts of face feature locations (eyes, nose, etc)\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     landmarks_as_tuples \u001b[38;5;241m=\u001b[39m [[(p\u001b[38;5;241m.\u001b[39mx, p\u001b[38;5;241m.\u001b[39my) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m landmark\u001b[38;5;241m.\u001b[39mparts()] \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m landmarks]\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\.venv\\lib\\site-packages\\face_recognition\\api.py:158\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    156\u001b[0m     face_locations \u001b[38;5;241m=\u001b[39m _raw_face_locations(face_image)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     face_locations \u001b[38;5;241m=\u001b[39m [_css_to_rect(face_location) \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n\u001b[0;32m    160\u001b[0m pose_predictor \u001b[38;5;241m=\u001b[39m pose_predictor_68_point\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\.venv\\lib\\site-packages\\face_recognition\\api.py:158\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    156\u001b[0m     face_locations \u001b[38;5;241m=\u001b[39m _raw_face_locations(face_image)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     face_locations \u001b[38;5;241m=\u001b[39m [\u001b[43m_css_to_rect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n\u001b[0;32m    160\u001b[0m pose_predictor \u001b[38;5;241m=\u001b[39m pose_predictor_68_point\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\.venv\\lib\\site-packages\\face_recognition\\api.py:49\u001b[0m, in \u001b[0;36m_css_to_rect\u001b[1;34m(css)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_css_to_rect\u001b[39m(css):\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    :return: a dlib `rect` object\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. _dlib_pybind11.rectangle(left: int, top: int, right: int, bottom: int)\n    2. _dlib_pybind11.rectangle(rect: dlib::drectangle)\n    3. _dlib_pybind11.rectangle(rect: _dlib_pybind11.rectangle)\n    4. _dlib_pybind11.rectangle()\n\nInvoked with: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     ]
    }
   ],
   "source": [
    "'''#pilteed_folder = \"../UTK_filtered\"\n",
    "test_face_c= \"../Color_Change\"\n",
    "os.makedirs(test_face_c, exist_ok=True)\n",
    "\n",
    "\n",
    "img_path = \"../UTK_filtered/white/28_0_0_20170104202019890.jpg.chip.jpg\"\n",
    "img_bgr = cv2.imread(img_path)\n",
    "if img_bgr is None:\n",
    "    raise ValueError(f\"Could not read image from {img_path}\")\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "face_locations = face_recognition.face_locations(img_bgr)\n",
    "print(type(img_rgb))\n",
    "\n",
    "\n",
    "# === Step 2. Detect face and get skin mask === #\n",
    "mask = skin_mask_from_landmarks(img_rgb, face_locations[0])\n",
    "if mask is None or mask.sum() == 0:\n",
    "    raise ValueError(\"No face detected or mask is empty!\")\n",
    "\n",
    "\n",
    "# === Step 3. Apply the skin color transformation === #\n",
    "# Example parameters: (ΔL, Δa, Δb) shifts in Lab space\n",
    "# You can tweak these to simulate lighter/darker or warmer/cooler skin tones\n",
    "target_lab_shift = np.array([-10, 8, 10])   # darker + slightly reddish tone\n",
    "\n",
    "changed_rgb = change_skin_color(img_rgb, mask, target_lab_shift)\n",
    "\n",
    "\n",
    "# === Step 4. Save and visualize results === #\n",
    "out_path = os.path.join(test_face_c, os.path.basename(img_path).replace(\".jpg\", \"_altered.jpg\"))\n",
    "cv2.imwrite(out_path, cv2.cvtColor(changed_rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# === Step 5. Display side-by-side comparison === #\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(img_rgb)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "axs[1].imshow(changed_rgb)\n",
    "axs[1].set_title(\"Altered Skin Tone\")\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\":turtle: Altered image saved to: {out_path}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e964ca",
   "metadata": {},
   "source": [
    "---\n",
    "# <h1 style=\"color:olive;\"><b>Test Facial Recogniton of Models</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c885880",
   "metadata": {},
   "source": [
    "do per model \n",
    "- for same colour but diff tonalities\n",
    "- check the diff clours toghether \n",
    "- compare lighter and darker tones\n",
    "- compare same colours but different races\n",
    "\n",
    "off all models \n",
    "- check ability to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c217ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c94851",
   "metadata": {},
   "source": [
    "---\n",
    "# <h1 style=\"color:olive;\"><b>RESULTS</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9307cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
