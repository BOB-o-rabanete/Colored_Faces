{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a517660a",
   "metadata": {},
   "source": [
    "---\n",
    "# <h1 style=\"color:olive;\">Library<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python numpy face_recognition lib-bin face_recognition_models scikit-image deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "'''import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from skimage import color\n",
    "from numpy.linalg import norm\n",
    "import math'''\n",
    "\n",
    "from functions import (get_face_embedding_fr, change_skin_color, \n",
    "                    cosine_similarity, \n",
    "                    get_subset_from_zip, abv_avg_quality_randomizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    img_path = args.input\n",
    "    out_prefix = args.out\n",
    "    target_rgb = tuple(int(x) for x in args.target.split(','))\n",
    "    strength = args.strength\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(\"Could not read image\", img_path)\n",
    "        return\n",
    "\n",
    "    # 1) Get baseline embedding\n",
    "    embedding, face_location = get_face_embedding_fr(img)\n",
    "    if embedding is None:\n",
    "        print(\"No face found. Try another image.\")\n",
    "        return\n",
    "    print(\"Baseline embedding found. Face location:\", face_location)\n",
    "\n",
    "    # Save a crop of the face for inspection\n",
    "    t, r, b, l = face_location\n",
    "    face_crop = img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_orig.jpg\", face_crop)\n",
    "\n",
    "    # 2) Pick a face with high chance of recognition:\n",
    "    # already using face_recognition gives an encoding; pick frontal/large bounding box images\n",
    "    # (preselect images yourself; this script processes one image)\n",
    "\n",
    "    # 3) Modify skin color\n",
    "    new_img = change_skin_color(img, face_location, target_rgb=target_rgb, strength=strength)\n",
    "    cv2.imwrite(f\"{out_prefix}_tinted.jpg\", new_img)\n",
    "    cv2.imwrite(f\"{out_prefix}_full_tinted.jpg\", new_img)\n",
    "\n",
    "    # 4) Get new embedding & compare\n",
    "    emb2, _ = get_face_embedding_fr(new_img)\n",
    "    if emb2 is None:\n",
    "        print(\"After transform, face not detected by the model.\")\n",
    "        # still save result and exit\n",
    "        return\n",
    "\n",
    "    # compute cosine similarity\n",
    "    sim = cosine_similarity(embedding, emb2)\n",
    "    # convert to distance proxy\n",
    "    dist = 1.0 - sim\n",
    "    print(f\"Cosine similarity between original & tinted embeddings: {sim:.4f}  (1-sim = {dist:.4f})\")\n",
    "\n",
    "    # Save face crops too\n",
    "    face_crop2 = new_img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_tinted.jpg\", face_crop2)\n",
    "\n",
    "    # Optional: print simple threshold check\n",
    "    threshold = 0.45  # typical face_recognition threshold for \"same\" varies by use-case\n",
    "    print(\"Similarity threshold (example):\", threshold)\n",
    "    if dist < threshold:\n",
    "        print(\"Model likely still recognizes as same person (dist < threshold).\")\n",
    "    else:\n",
    "        print(\"Model may no longer consider it the same (dist >= threshold).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"input image path\")\n",
    "    parser.add_argument(\"out\", help=\"output prefix\")\n",
    "    parser.add_argument(\"--target\", default=\"255,200,0\", help=\"target RGB as 'R,G,B' (0-255)\")\n",
    "    parser.add_argument(\"--strength\", type=float, default=0.85, help=\"0..1 how strong the tint is\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ccc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <h1 style=\"color:olive;\">CREATE DATASET<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57719a4d",
   "metadata": {},
   "source": [
    "### to remove/alter\n",
    "\n",
    "Choose a face and compute a high-confidence embedding (we show how to pick a face with a single clear detection).\n",
    "\n",
    "Dataset: UTKFace (easiest + labeled by ethnicity) imgs are classified [age]_[gender]_[race]_[date&time].jpg\n",
    "\n",
    "Sample size: 50 (10 White, 10 Black, 10 Asian, 10 Indian, 10 Latino)\n",
    "\n",
    "Control: Same sex, similar age range (20–30)\n",
    "\n",
    "Use case: Color variation bias experiment\n",
    "\n",
    "Ethical note: Only public research datasets, no scraped Google images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199db5a",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:olive;\">Baseline Dataset</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd3db5",
   "metadata": {},
   "source": [
    "In this stage of the ptoject, a baseline image dataset is constructed using the **UTKFace dataset**, a publicly available facial image dataset commonly used for research in facial recognition, demographic analysis and computer vision.\n",
    "The UTKFace dataset contains over 22 000 face images labeled accordinge to age, gender, race and date&time in the filename format: [age]_[gender]_[race]_[date&time].jpg\n",
    "- [age]: integer form 0 to 116, indicating age\n",
    "- [gender]: either 0(male) or 1(female)\n",
    "- [race]: integer from 0 to 4, denoting White, Black, Asian, Indian, and Others(like Hispano, Latino, Middle Eastern)\n",
    "- [date&time]: format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace\n",
    "\n",
    "Regarding data use and ethics, the UTKFace dataset is distributed for non-commercial, academic research purposes, and includes images collected under fair-use principles. While the dataset contains real human faces, its usage is generally considered legally permissible provided it is handled responsibly, with appropriate anonymization and without any attempt to identify or misuse the individuals depicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs --- #\n",
    "df_path = r\"C:/Users/Daniela/Desktop/Fac/M.IA/ano_1/semestre_1/IAS/Projeto_Individual/UTKFace_zipedfolder.zip\"\n",
    "\n",
    "filteed_folder = \"../UTK_filtered\"\n",
    "target_age_range = (28, 34)\n",
    "target_gender = 0 \n",
    "SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3bbd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2197 images of (28 to 34)-year-old male.\n",
      " Successfully extracted 2197 images to '../UTK_filtered'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2197"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subset_from_zip(df_path, filteed_folder, target_age_range, target_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a3d7f",
   "metadata": {},
   "source": [
    "Whilst verifying the imported image dataset, it was found that a small subset of images was misclassified. Some of those fotos were incorrectrly labeled in terms of gender(female images appeared on the male category) and ethnicity(Black individuals categorized under the 'White' group)  \n",
    "\n",
    "To ensure higher data accuray and consistency, it was added a manual process were any obvious outliars were removed. It was also removed any black and white only foto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e810856",
   "metadata": {},
   "source": [
    "### to remove\n",
    "\n",
    "Note: even though India is in Asia, due to the glaringly diferenceds noticed between Arabians, Indians, Russians to the rest of Asia (Indonisia, China, Mongolia,...) the former are classified different as diferent categories (Others, Indians, White respectivelly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86291c84",
   "metadata": {},
   "source": [
    "filtering “high-quality” images per race\n",
    "Measure image quality — usually by sharpness (variance of Laplacian) or brightness/contrast balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add part where u get avrg img quality per group and choose random 10 form img that are above avrg quality\n",
    "# those imgs will have their names saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f57e75",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:olive;\">Alter Skin Pigmentation</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3e847",
   "metadata": {},
   "source": [
    "Change the skin color programmatically with a function that (a) finds a skin-region mask from facial landmarks and (b) shifts the skin pixels toward a target color (any RGB you pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e964ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Facial Recogniton of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c885880",
   "metadata": {},
   "source": [
    "do per model \n",
    "- for same colour but diff tonalities\n",
    "- check the diff clours toghether \n",
    "- compare lighter and darker tones\n",
    "- compare same colours but different races\n",
    "\n",
    "off all models \n",
    "- check ability to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c217ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c94851",
   "metadata": {},
   "source": [
    "---\n",
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9307cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
