{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a517660a",
   "metadata": {},
   "source": [
    "---\n",
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python numpy face_recognition lib-bin face_recognition_models scikit-image deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b135beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import argparse\n",
    "from skimage import color\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "from functions import (get_face_embedding_fr, change_skin_color, \n",
    "                        cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a73dbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--target TARGET] [--strength STRENGTH]\n",
      "                             input out\n",
      "ipykernel_launcher.py: error: the following arguments are required: input, out\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniela\\Desktop\\Fac\\M.IA\\ano_1\\semestre_1\\IAS\\Projeto_Individual\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    img_path = args.input\n",
    "    out_prefix = args.out\n",
    "    target_rgb = tuple(int(x) for x in args.target.split(','))\n",
    "    strength = args.strength\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(\"Could not read image\", img_path)\n",
    "        return\n",
    "\n",
    "    # 1) Get baseline embedding\n",
    "    embedding, face_location = get_face_embedding_fr(img)\n",
    "    if embedding is None:\n",
    "        print(\"No face found. Try another image.\")\n",
    "        return\n",
    "    print(\"Baseline embedding found. Face location:\", face_location)\n",
    "\n",
    "    # Save a crop of the face for inspection\n",
    "    t, r, b, l = face_location\n",
    "    face_crop = img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_orig.jpg\", face_crop)\n",
    "\n",
    "    # 2) Pick a face with high chance of recognition:\n",
    "    # already using face_recognition gives an encoding; pick frontal/large bounding box images\n",
    "    # (preselect images yourself; this script processes one image)\n",
    "\n",
    "    # 3) Modify skin color\n",
    "    new_img = change_skin_color(img, face_location, target_rgb=target_rgb, strength=strength)\n",
    "    cv2.imwrite(f\"{out_prefix}_tinted.jpg\", new_img)\n",
    "    cv2.imwrite(f\"{out_prefix}_full_tinted.jpg\", new_img)\n",
    "\n",
    "    # 4) Get new embedding & compare\n",
    "    emb2, _ = get_face_embedding_fr(new_img)\n",
    "    if emb2 is None:\n",
    "        print(\"After transform, face not detected by the model.\")\n",
    "        # still save result and exit\n",
    "        return\n",
    "\n",
    "    # compute cosine similarity\n",
    "    sim = cosine_similarity(embedding, emb2)\n",
    "    # convert to distance proxy\n",
    "    dist = 1.0 - sim\n",
    "    print(f\"Cosine similarity between original & tinted embeddings: {sim:.4f}  (1-sim = {dist:.4f})\")\n",
    "\n",
    "    # Save face crops too\n",
    "    face_crop2 = new_img[t:b, l:r]\n",
    "    cv2.imwrite(f\"{out_prefix}_face_tinted.jpg\", face_crop2)\n",
    "\n",
    "    # Optional: print simple threshold check\n",
    "    threshold = 0.45  # typical face_recognition threshold for \"same\" varies by use-case\n",
    "    print(\"Similarity threshold (example):\", threshold)\n",
    "    if dist < threshold:\n",
    "        print(\"Model likely still recognizes as same person (dist < threshold).\")\n",
    "    else:\n",
    "        print(\"Model may no longer consider it the same (dist >= threshold).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"input image path\")\n",
    "    parser.add_argument(\"out\", help=\"output prefix\")\n",
    "    parser.add_argument(\"--target\", default=\"255,200,0\", help=\"target RGB as 'R,G,B' (0-255)\")\n",
    "    parser.add_argument(\"--strength\", type=float, default=0.85, help=\"0..1 how strong the tint is\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ccc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CREATE DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199db5a",
   "metadata": {},
   "source": [
    "## Get default faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fa709",
   "metadata": {},
   "source": [
    "Choose a face and compute a high-confidence embedding (we show how to pick a face with a single clear detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f57e75",
   "metadata": {},
   "source": [
    "## Change face colours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3e847",
   "metadata": {},
   "source": [
    "Change the skin color programmatically with a function that (a) finds a skin-region mask from facial landmarks and (b) shifts the skin pixels toward a target color (any RGB you pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e964ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Facial Recogniton of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c885880",
   "metadata": {},
   "source": [
    "do per model \n",
    "- for same colour but diff tonalities\n",
    "- check the diff clours toghether \n",
    "- compare lighter and darker tones\n",
    "- compare same colours but different races\n",
    "\n",
    "off all models \n",
    "- check ability to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c217ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
